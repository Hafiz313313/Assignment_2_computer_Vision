{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2165ed38",
   "metadata": {},
   "source": [
    "# Computer Vision, Assignment 2: Calibration and DLT\n",
    "\n",
    "In this assignment you will study camera calibration, the projective ambiguity, and the DLT method. \n",
    "\n",
    "You will solve the resection and triangulation problems using DLT and compute inner parameters using RQ factorization.\n",
    "In addition you will try out SIFT for feature detection/matching.\n",
    "\n",
    "Please see Canvas for detailed instructions on what is expected for a passing/higher grade. All computer exercises not marked **OPTIONAL** are \"mandatory\" in the sense described on Canvas.\n",
    "\n",
    "### Submission Requirements:\n",
    "Your final lab submission should include:\n",
    "1. Your edited **notebook file** (`.ipynb`).\n",
    "2. An **HTML printout** of the executed notebook with all outputs visible: File → Save and export Notebook As → HTML\n",
    "3. A **pdf report** containing answers to the theoretical exercises (see separate document).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2573ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating responsive plots\n",
    "%matplotlib widget  \n",
    "\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "\n",
    "from supplied import pflat, plot_camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c5283",
   "metadata": {},
   "source": [
    "# Calibrated vs. Uncalibrated Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d7a65-50e6-4f5c-9a7b-c508a00286fd",
   "metadata": {},
   "source": [
    "#### *Theoretical exercise 1* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46690e17-96cc-418d-b6f4-47977a8d7554",
   "metadata": {},
   "source": [
    "## Computer Exercise 1\n",
    "Figure 1 shows an image of a scene and a reconstruction using uncalibrated cameras. The file `compEx1data.mat` contains \n",
    "the 3D points of the reconstruction `X`, the camera matrices `P`, the image points `x` and the filenames `imfiles` of the images.\n",
    "Here `X` is a $4 \\times 9471$ matrix containing the homogeneous coordinates for all 3D points, `x{i}` is a $3 \\times 9471$ matrix containing the homogeneous coordinates of the image points seen in image $i$ (`NaN` means that the point has not been detected in this image). `P{i}` contains the camera matrix of image $i$ and `imfiles{i}` contains the name of that image.\n",
    "\n",
    "<figure align=\"center\">\n",
    "    <img alt=\"left\" src=\"figs/DSC_0025.JPG\" width=\"250px\">\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;\n",
    "    <img alt=\"right\" src=\"figs/projective_rec.png\" width=\"150px\">\n",
    "    <figcaption>Figure 1: Left: An image of the scene. Right: A projective (uncalibrated) reconstruction.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b2301-1794-485f-a753-b71fde4f2759",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "Plot the 3D points of the reconstruction. Use the supplied function `plot_camera` to plot the cameras in the same figure.\n",
    "\n",
    "**Do the physical properties (such as the relative heights of each of the two walls, the angle in the corner, and the relation between the width, height, and depth dimensions) look realistic in the reconstruction?**\n",
    "\n",
    "Don't forget to use `ax.set_aspect('equal')` when plotting -- otherwise you may get additional distortion. The reconstruction should look like the one shown in Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63570-43ae-410a-a48f-53d173e6141d",
   "metadata": {},
   "source": [
    "#### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dac7df",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "Project the 3D points into **a camera** of your choice (only points that have been detected in that camera).\n",
    "Plot the image, the projected points, and the image points in the same figure.\n",
    "\n",
    "**Do the projections appear to be close to the corresponding image points?** (If not: Did you forget to divide by the third coordinate?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af49983",
   "metadata": {},
   "source": [
    "**Useful python commands:**\n",
    "\n",
    "`~np.isnan(x_i)` # to check which of the points are visible in image i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e887566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608ad92",
   "metadata": {},
   "source": [
    "#### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b72486",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "Using the two projective transformations \n",
    "\\begin{equation}\n",
    "T_1 = \\left( \\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0\\\\\n",
    "0 & 3 & 0 & 0\\\\ \n",
    "0 & 0 & 1 & 0\\\\\n",
    "1/8 & 1/8 & 0 & 1\n",
    "\\end{array} \\right) \\text{ and }\n",
    "T_2 = \\left( \\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\ \n",
    "0 & 0 & 1 & 0\\\\\n",
    "1/16 & 1/16 & 0 & 1\n",
    "\\end{array} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "transform the 3D points ($\\mathbf{X} \\mapsto T_i\\mathbf{X}$), and accordingly the cameras (as in *Theoretical Exercise 1*), so that two new projective solutions are obtained.\n",
    "\n",
    "For each of the solutions, plot the 3D points and cameras in the same figure. (Don't forget to divide the points by the fourth coordinate before plotting, and again don't forget to use `ax.set_aspect('equal')`. Feel free to use the provided `pflat` function.)\n",
    "\n",
    "**What has happened to the 3D points?**\n",
    "**Does any of them appear reasonable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15900f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b62d9",
   "metadata": {},
   "source": [
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ae7b0",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "For one of the 9 views, and for each of the new reconstructions (resulting from $T_1$ and $T_2$), project the new 3D points into its corresponding transformed camera.\n",
    "Plot the pair of images along with the corresponding projected points, and the image points in the same figure. \n",
    "(Write a function `project_and_plot(P, Xs, xs, image)` that does this for you, it will be useful in the future.)\n",
    "\n",
    "**How well do the projections and the image points align for the new reconstructions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea21c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_and_plot(P, Xs, xs, image):\n",
    "    # Your code here\n",
    "    \n",
    "\n",
    "# Project and plot the reconstruction resulting from T_1\n",
    "\n",
    "\n",
    "# Project and plot the reconstruction resulting from T_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d85ad",
   "metadata": {},
   "source": [
    "#### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1591b9",
   "metadata": {},
   "source": [
    "#### *Theoretical exercises: 2* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43460d4d-ca2d-466a-a631-4f9d83ee6641",
   "metadata": {},
   "source": [
    "# Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d9c35",
   "metadata": {},
   "source": [
    "#### *Theoretical exercises: 3* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc071eca",
   "metadata": {},
   "source": [
    "# RQ Factorization and Computation of $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda56d6",
   "metadata": {},
   "source": [
    "#### *Theoretical exercises: 4* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360fa1f",
   "metadata": {},
   "source": [
    "# Direct Linear Transformation DLT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd5444",
   "metadata": {},
   "source": [
    "#### *Theoretical exercises: 5 and 6* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fafde",
   "metadata": {},
   "source": [
    "## Computer Exercise 2\n",
    "Figure 2 shows two images `cube1.JPG` and `cube2.JPG` of a scene with a Rubik's cube. \n",
    "The file `compEx2data.mat` contains a point model `Xmodel` of the visible cube sides, the measured projections `x` of the model points\n",
    "in the two images and two variables `startind`, `endind` that can be used for plotting lines on the model surface.\n",
    "\n",
    "<figure align=\"center\">\n",
    "    <img alt=\"left\" src=\"figs/cube1.JPG\" width=\"250px\">\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;\n",
    "    <img alt=\"right\" src=\"figs/cube2.JPG\" width=\"250px\">\n",
    "    <figcaption>Figure 2: Two images of a scene containing a Rubik's cube.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The goal for this exercise is to determine the camera for each of the two views, i.e. resectioning.\n",
    "*You should consider one view at a time*, i.e. apply your code for each view separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cca25e",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "Normalize the measured points by applying a transformation $N$ that subtracts the mean of the points and then re-scales the coordinates by the standard deviation.\n",
    "Here, compute mean and standard deviation for $x$ and $y$ separately (separating the coordinates is not crucial, but it helps during grading if you do this in a coherent way).\n",
    "\n",
    "Plot the normalized points in a new figure. Verify that the points are centered around $(0,0)$ with standard deviation 1 for each of the coordinates.\n",
    "\n",
    "Remember to print out the mean and standard deviation you used for normalizing x and y, for each camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad6d789a-1925-4521-8b51-9b8390cce79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f1cb8",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "Implement a function `estimate_camera_DLT` that sets up the DLT equations for resectioning, and solves the resulting homogeneous least squares system using SVD.\n",
    "\n",
    "$$\\min_{||v||^2 = 1}||Mv||^2$$\n",
    "\n",
    "**Is the smallest singular value close to zero? How about $||Mv||$?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e384fc",
   "metadata": {},
   "source": [
    "**Useful python commands:**\n",
    "\n",
    "`U, S, Vt = np.linalg.svd(M)` # compute SVD of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_camera_DLT(x, Xmodel):\n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda317fd",
   "metadata": {},
   "source": [
    "#### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c46fe",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "Extract the entries of the camera from the solution and set up the camera matrix.\n",
    "Make sure that you select the solution where the points are in front of the camera.\n",
    "(If $X$ has 4th coordinate 1 then the 3rd coordinate of $PX$ should be positive for $X$ to be in front of the camera.)\n",
    "\n",
    "Project the model points into the images. (Don't forget to transform the camera matrix to the original (un-normalized) coordinate system, as in *Theoretiacl Exercise 6*, before projecting.) Plot the measured image points in the same figure. **Are they close to each other?** Also make a 3D plot visualizing the camera centers, principal axes, as well as the 3D model points.\n",
    "**Does the result look reasonable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bbfaa052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------ \n",
    "\n",
    "# extract the camera matrices\n",
    "\n",
    "# Project and plot the models points into the images\n",
    "\n",
    "# Make a 3D plot with 3D model points, camera centers, and principal axes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0ee0a",
   "metadata": {},
   "source": [
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e248ee2",
   "metadata": {},
   "source": [
    "### Task 2.4\n",
    "Compute the inner parameters of the first camera using RQ decomposition. \n",
    "\n",
    "**How can we know that these are the \"true\" parameters? Why is there no ambiguity as in Theoretical Exercise 1?**\n",
    "\n",
    "Please also print out the calibration matrix of the first camera, normalized s.t. $K_{33}=1$.\n",
    "\n",
    "When you have achieved satisfactory results, save the camera matrices (for both views) to be used in further exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2c26",
   "metadata": {},
   "source": [
    "**Useful python commands:**\n",
    "\n",
    "`scipy.linalg.rq(P)` # compute the RQ decomposition of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d7a9f",
   "metadata": {},
   "source": [
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a1974",
   "metadata": {},
   "source": [
    "### Task 2.5 (OPTIONAL, 10 optional points)\n",
    "You will now re-run your experiments in a few different settings to be compared, investigating the importance of normalization. You only need to do this part for **the first view** (corresponding to 2D points `x{1}`). For each experiment, measure the performance with the RMS error \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "e_{RMS} = \\sqrt{ \\frac{1}{n} ||x_{meas} - x_{proj}||_F^2 },\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $x_{meas}$, $x_{proj}$ are the Cartesian coordinates ($2\\times n$-matrices) for the measured points and projected model points respectively, and $n$ is the number of points. Here $|| \\cdot ||_F^2$ denotes the (squared) Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276795f",
   "metadata": {},
   "source": [
    "**Useful python commands:**\n",
    "\n",
    "`np.linalg.norm(dx, 'fro') ** 2` # compute the squared Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_rms(x_measured, x_projected):\n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730fea4",
   "metadata": {},
   "source": [
    "First compute RMS values when using normalization (as you have done so far).\n",
    "Next, repeat the whole resectioning experiment one more time, but this time don't normalize the points. (The easiest way of doing this is probably to use $N=I$ as normalization matrix and run the same code again.)\n",
    "\n",
    "**Is the difference large?** \n",
    "\n",
    "Report the RMS number and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b66f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8eee8",
   "metadata": {},
   "source": [
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62325423",
   "metadata": {},
   "source": [
    "Now repeat the resectioning experiments (with and without normalization), but this time estimate the camera using only points number $1,4,13,16,25,28,31$. Still, measure the RMS error in a unified way, always using all points!.\n",
    "\n",
    "**What can you conclude from the experiments that you have run?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ded2d4",
   "metadata": {},
   "source": [
    "\n",
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa078a",
   "metadata": {},
   "source": [
    "# Feature Extraction and Matching using SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c8341",
   "metadata": {},
   "source": [
    "## Computer Exercise 3\n",
    "In this exercise you will get to try feature extraction using SIFT.\n",
    "\n",
    "You will need `OpenCV` package for computing SIFT features so first make sure that you have the package installed and successfully imported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd088460",
   "metadata": {},
   "source": [
    "### Task 3.1\n",
    "First load the images `cube1.jpg` and `cube2.jpg` from Computer Exercise 2. We will work with **grayscale** images for SIFT.\n",
    "\n",
    "Compute SIFT features using OpenCV’s implementation of SIFT. \n",
    "\n",
    "**Useful python commands:**\n",
    "\n",
    "`sift = cv2.SIFT_create(contrastThreshold=0.01)` # initialize a sift detector\n",
    "\n",
    "`keypoints1, descriptors1 = sift.detectAndCompute(im1, None)` # compute keypoints and descriptors\n",
    "\n",
    "\n",
    "The SIFT detector searches for peaks in scale space (similar to peaks in the autocorrelation function, see lecture notes). The `contrastThreshold` parameter filters out weak peaks.\n",
    "\n",
    "The variable `keypoints1` contains a list of detected features `kp`. Each keypoint has: \n",
    "- `kp.pt` -- $(x, y)$ coordinates\n",
    "- `kp.angle` -- an orientation\n",
    "- `kp.size` -- a scale\n",
    "\n",
    "and the descriptor in `descriptors1` encodes local image information around that point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n",
    "\n",
    "# load images in greyscale\n",
    "\n",
    "# compute keypoints and descriptors for both images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89224535",
   "metadata": {},
   "source": [
    "Visualize the detected keypoints with the images.\n",
    "```\n",
    "im1_kp = cv2.drawKeypoints(im1, keypoints1, None,\n",
    "                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ab3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824502da",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3.2\n",
    "We now want to find correspondences between the two sets of SIFT descriptors.\n",
    "\n",
    "```\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "```\n",
    "\n",
    "For each descriptor in the first image, `bf.knnMatch.knnMatch()` returns `k` best matches in the second image, where `k=2` by default. \n",
    "\n",
    "However, sometimes a feature may have two or more similarly good matches, especially in textured or repetitive regions — these are ambiguous and should not be trusted.\n",
    "\n",
    "We can apply **Lowe's ratio test** to filter out these ambiguous matches by comparing the distance of the best match (`m`) to the second-best match (`n`) and keep only the matches if the best match is much better than the second best (e.g. `m.distance < 0.75 * n.distance`).\n",
    "\n",
    "Otherwise, both matches are almost equally good, the match is ambiguous, and we should reject it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n",
    "\n",
    "# Compute matches between descriptors1 and descriptors2\n",
    "\n",
    "# Apply Lowe's ratio test to filter out the ambiguous mathes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6629d4",
   "metadata": {},
   "source": [
    "### Task 3.3\n",
    "We can now extract matching points as follows. (Note that you will need the point sets $x1$ and $x2$ for the next exercise.)\n",
    "```\n",
    "x1 = np.array([keypoints1[m.queryIdx].pt for m in good_matches]).T\n",
    "x2 = np.array([keypoints2[m.trainIdx].pt for m in good_matches]).T\n",
    "```\n",
    "\n",
    "Randomly select 10 matches, plot the two images next to each other and plots lines between the matching points.\n",
    "\n",
    "**How many of the matches appear to be correct?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d92a3",
   "metadata": {},
   "source": [
    "**Useful python commands:**\n",
    "\n",
    "```\n",
    "im_matches = cv2.drawMatchesKnn(im1, keypoints1,\n",
    "                                im2, keypoints2,\n",
    "                                selected_matches, None,\n",
    "                                flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc587949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n",
    "\n",
    "# Extract matching points\n",
    "\n",
    "# Plot matching points on the two images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efdc85",
   "metadata": {},
   "source": [
    "\n",
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011fbe4",
   "metadata": {},
   "source": [
    "# Triangulation using DLT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976de50",
   "metadata": {},
   "source": [
    "## Computer Exercise 4\n",
    "Using the estimated cameras from Computer Exercise 2 you will now triangulate the points detected in Computer Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f47dd5",
   "metadata": {},
   "source": [
    "### Task 4.1\n",
    "Implement a function `triangulate_3D_point_DLT` that set ups the DLT equations for triangulation, and solves the homogeneous least squares system.\n",
    "(You will have to do this in a loop, once for each point.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ac4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_3D_point_DLT(x1, x2, P1, P2):\n",
    "    # Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8d73b",
   "metadata": {},
   "source": [
    "Project the computed points into the two images and compare with the corresponding SIFT-points in a 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n",
    "\n",
    "# Compute the 3D points\n",
    "\n",
    "# Project the computed points into the two images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358241c",
   "metadata": {},
   "source": [
    "### Task 4.2\n",
    "Now, re-run the triangulation, but first normalize 2D points and cameras using the inverse of $K$. Again, compare with the SIFT points in a plot.\n",
    "Just as when normalizing with $N$ (based on mean/std) for the resection problem in Computer Exercise 2, how is your result for triangulation from normalization compared to the result without normalization? Note that using $K$ or $N$ for normalization doesn't matter much, but $K$ was not available for the previous problem (resection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ebad6",
   "metadata": {},
   "source": [
    "### Task 4.3\n",
    "From now on, consider only the normalized case.\n",
    "\n",
    "As we saw in Computer Exercise 3, a portion of the SIFT matches will be incorrect. Most of the time (but not always) this will result in triangulations with large error.\n",
    "Compute the average pixel error (for each of the two images) between the projected 3D points and the corresponding SIFT points (remember to multiply with $K$ to retrieve pixel coordinates rather than normalized coordinates).\n",
    "\n",
    "Remove those points for which the error in at least one of the images is larger than $3$ pixels.\n",
    "Plot the remaining 3D points, the cameras and the cube model in one and the same 3D plot. \n",
    "\n",
    "**Can you distinguish the dominant objects (the cups and the paper)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af56955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Your code here ------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0bede9",
   "metadata": {},
   "source": [
    "\n",
    "### Your answer here: \n",
    "\n",
    "(*Include answers to all questions marked in bold.*)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
